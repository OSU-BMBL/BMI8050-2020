{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCNet - A simple LSTM-RNN for generating sequence consensus\n",
    "We will explore building a long short term memory (LSTM) for consensus sequence generation. We will train a model to learn the patterns in simulated noise sequences and then use this model to generate the consensus sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dependencies\n",
    "First, we need some boilerplate code to load the PyTorch and other modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataset\n",
    "Generate a random sequence of length 220 as our original teamplate sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = [random.choice([\"A\",\"C\",\"G\",\"T\"]) for _ in range(220)]\n",
    "print(\"\".join(seq))\n",
    "# convert the `seq` to a PyTorch tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a couple utility functions for encoding DNA bases to vectors and simulate sequencing errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_error(seq, pi=0.05, pd=0.05, ps=0.01):\n",
    "    \"\"\"\n",
    "    Given an input sequence `seq`, generating another\n",
    "    sequence with errors. \n",
    "    pi: insertion error rate\n",
    "    pd: deletion error rate\n",
    "    ps: substitution error rate\n",
    "    \"\"\"\n",
    "    out_seq = []\n",
    "    for c in seq:\n",
    "        while 1:\n",
    "            r = random.uniform(0,1)\n",
    "            if r < pi:\n",
    "                out_seq.append(random.choice([\"A\",\"C\",\"G\",\"T\"]))\n",
    "            else:\n",
    "                break\n",
    "        r -= pi\n",
    "        if r < pd:\n",
    "            continue\n",
    "        r -= pd\n",
    "        if r < ps:\n",
    "            out_seq.append(random.choice([\"A\",\"C\",\"G\",\"T\"]))\n",
    "            continue\n",
    "        out_seq.append(c)\n",
    "    return \"\".join(out_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmap = {\"A\":0, \"C\":1, \"G\":2, \"T\":3}\n",
    "def one_hot(b):\n",
    "    t = [[0,0,0,0]]\n",
    "    i = bmap[b]\n",
    "    t[0][i] = 1\n",
    "    return t\n",
    "\n",
    "print(\"one-hot encoding for DNA bases\")\n",
    "print(\"A:\", one_hot(\"A\"))\n",
    "print(\"C:\", one_hot(\"C\"))\n",
    "print(\"G:\", one_hot(\"G\"))\n",
    "print(\"T:\", one_hot(\"T\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_t = Variable(torch.FloatTensor([one_hot(c) for c in seq])).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate 20 erroneous sequences with insertion error at 5%, deletion errors at 5% and subsitution error at 1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seqs = [sim_error(seq, pi=0.05, pd=0.05, ps=0.01) for _ in range(20)]\n",
    "seqs_t = [Variable(torch.FloatTensor([one_hot(c) for c in s])).cuda()  for s in seqs]\n",
    "seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model construction\n",
    "Here we define the simple DCNet class as subclass of `torch.nn.Module`.\n",
    "\n",
    "Fill in the TODOs to define the RNN model within the build_model function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCNet(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim, layer1_dim, layer2_dim):\n",
    "        super(DCNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        '''TODO: One LSTM layer that needs to specify the input demension, hidden layer demension, as well as the number of recurrent layers.\n",
    "        Here, we use a four demensional one-hot encoding as the input.'''\n",
    "        self.lstm = nn.LSTM(input_size = '''TODO''', hidden_size = hidden_dim, num_layers = '''TODO''') \n",
    "        \n",
    "        # Two fully connected layers with ReLU as activation function\n",
    "        self.linear1 = nn.Linear(in_features = hidden_dim, out_features = layer1_dim)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        '''TODO: add the second fully connected layer with ReLU as the activation function'''\n",
    "        self.linear2 = '''TODO'''\n",
    "        \n",
    "        \n",
    "        # Output layer\n",
    "        '''TODO: specify the output demension.\n",
    "        Here, the input is a vector of four demension'''\n",
    "        self.linear3 = nn.Linear(in_features = '''TODO''', out_features = '''TODO''')\n",
    "        \n",
    "        # Weight initialization\n",
    "        self.hidden_init_values = None\n",
    "        self.hidden = self.init_hidden()\n",
    "        nn.init.xavier_uniform(self.linear1.weight)\n",
    "        nn.init.xavier_uniform(self.linear2.weight)\n",
    "        nn.init.xavier_uniform(self.linear3.weight)\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        if self.hidden_init_values == None:\n",
    "            self.hidden_init_values = (autograd.Variable(torch.randn(1, 1, self.hidden_dim)),\n",
    "                                       autograd.Variable(torch.randn(1, 1, self.hidden_dim)))\n",
    "        return self.hidden_init_values\n",
    "\n",
    "    # Forward process\n",
    "    def forward(self, seq):\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            seq.view(len(seq), 1, -1), self.hidden)\n",
    "        tmp1 = self.relu1(self.linear1(lstm_out.view(len(seq), -1)))\n",
    "        tmp2 = self.relu2(self.linear2(tmp1))\n",
    "        _out = self.linear3(tmp2)\n",
    "        base_out = _out\n",
    "        return base_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the DCNet. You can use different parameters to initilize the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''TODO: initialize the neurons in hidden layers. such as 32 in LSTM layer, and 12 neurons in each fully connetcted layers'''\n",
    "dcnet = DCNet(hidden_dim = '''TODO''', layer1_dim = '''TODO''', layer2_dim = '''TODO''')\n",
    "\n",
    "# Set the gradients manually to zero\n",
    "dcnet.zero_grad()\n",
    "dcnet.cuda()\n",
    "# Initial the paramerters in the DCNet\n",
    "dcnet.hidden = dcnet.init_hidden()\n",
    "for name, param in dcnet.named_parameters():\n",
    "    if 'bias' in name:\n",
    "        nn.init.constant(param, 0.0)\n",
    "    elif 'weight' in name:\n",
    "        nn.init.xavier_normal(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Training the model: loss and training operations\n",
    "Set up the loss function and SGD optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use mean square error as the loss function. You can use mean absolute error as the loss function by the function nn.L1Loss()  \n",
    "# loss_function = nn.L1Loss()\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "'''TODO: Initilize the leaning rate as 0.1'''\n",
    "lr = '''TODO'''\n",
    "\n",
    "# Use stochastic gradient descent to train the model\n",
    "optimizer = optim.SGD(dcnet.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following loop train the DCnet using the noisy sequences. We also lowe the learning rate `lr` every 250 epoches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "range_ = (1, 200)\n",
    "# Mini-batch: a subset of the training set that is used to evaluate the gradient of the loss function and update the weights. \n",
    "mini_batch_size = 5\n",
    "\n",
    "# One epoch: an entire dataset is passed forward and backward through the neural network exactly one time. \n",
    "'''TODO: change the epoch to 2001'''\n",
    "for epoch in range('''TODO'''):\n",
    "    for i in range(int(len(seqs_t)/mini_batch_size)):\n",
    "        loss = 0\n",
    "        s, e = range_\n",
    "        for seq2 in random.sample(seqs_t, mini_batch_size):\n",
    "            dcnet.hidden = dcnet.init_hidden()\n",
    "            dcnet.zero_grad()\n",
    "            seq2 = seq2[s-1:e]\n",
    "            seq_ = seq2.view(-1,4)\n",
    "            out = dcnet(seq_)\n",
    "            loss += loss_function(out[:-1], seq_[1:])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 250==0:\n",
    "        print(\"epoch:\", epoch, \"loss:\", loss.cpu().data/mini_batch_size, \"learning rate:\", lr)\n",
    "        \n",
    "        # Use a decreasing learning rate to converge to the optimal value. \n",
    "        '''TODO: the decreasing rate is set to 0.95 '''\n",
    "        lr *= '''TODO'''\n",
    "        optimizer = optim.SGD(dcnet.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Result visualization \n",
    "We can visualize the input and output tensors after we send the whole original template into the DCNet to reconstruct the output tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "dcnet.hidden = dcnet.init_hidden()\n",
    "xout = dcnet(seq_t[:250])\n",
    "x1 = xout[:-1].cpu().data.numpy() \n",
    "xx1=np.transpose(seq_t[1:250,0,:].data.cpu().numpy())\n",
    "xx2=np.transpose(x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code snippet using DCNet to generate a consensus sequence with only the first base in the template. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcnet.hidden = dcnet.init_hidden()\n",
    "base_t= Variable(torch.FloatTensor([one_hot(seq[0])])).cuda()\n",
    "\n",
    "consensus = []\n",
    "\n",
    "for _ in range(201):\n",
    "    xout = dcnet(base_t)\n",
    "    next_t = [0,0,0,0]\n",
    "    next_t[np.argmax(xout.cpu().data.numpy())]=1\n",
    "    consensus.append(next_t)\n",
    "    base_t= Variable(torch.FloatTensor([next_t])).cuda()\n",
    "consensus = np.array(consensus)\n",
    "consensus = consensus.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the generated consensus to the original template. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,3))\n",
    "plt.subplot(3,1,1)\n",
    "plt.matshow(xx1[:,0:201], vmin=-0.1, vmax=1.1, fignum=False)\n",
    "plt.text(0,6,\"the tensor for the original template\")\n",
    "frame = plt.gca()\n",
    "frame.axes.yaxis.set_ticklabels([])\n",
    "\n",
    "plt.subplot(3,1,2)\n",
    "plt.matshow(consensus, vmin=-0.1, vmax=1.1, fignum=False)\n",
    "plt.text(0,6,\"generated consensus from the DCNet\")\n",
    "frame = plt.gca()\n",
    "frame.axes.yaxis.set_ticklabels([])\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "plt.matshow(consensus-xx1[:,0:201], vmin=-0.1, vmax=1.1, fignum=False)\n",
    "plt.text(0,6,\"differences\")\n",
    "frame = plt.gca()\n",
    "frame.axes.yaxis.set_ticklabels([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below plots three matrices. The first one is from the original template. The second one is the reconstruction from the the DCNet using the first base in the template as input. The third one is the differences between the two matrices. If we have generate the consensus perfectly matching the original template, the elements of the third matrix will be all zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bmi8050 [~/.conda/envs/bmi8050/]",
   "language": "python",
   "name": "conda_bmi8050"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
